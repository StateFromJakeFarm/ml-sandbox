{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop From Scratch\n",
    "Following this [post](https://towardsdatascience.com/backpropagation-made-easy-e90a4d5ede55)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from numpy.random import normal\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_dim, layer_dims):\n",
    "        dims = [input_dim] + layer_dims\n",
    "\n",
    "        # Weights\n",
    "        self.W = []\n",
    "        for i in range(1, len(dims)):\n",
    "            # Begin with random weights\n",
    "            self.W.append( normal(size=(dims[i], dims[i-1])) )\n",
    "\n",
    "        # Biases\n",
    "        self.b = [np.zeros((d, 1)) for d in layer_dims]\n",
    "\n",
    "        # Weighted sums\n",
    "        self.z = [np.zeros((d, 1)) for d in layer_dims]\n",
    "\n",
    "        # Weighted sum activation values\n",
    "        self.a = [np.zeros((d, 1)) for d in layer_dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NN(2, [3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, input_dim, layer_dims):\n",
    "        dims = [input_dim] + layer_dims\n",
    "\n",
    "        # Weights\n",
    "        self.W = []\n",
    "        for i in range(1, len(dims)):\n",
    "            # Begin with random weights\n",
    "            self.W.append( normal(size=(dims[i], dims[i-1])) )\n",
    "\n",
    "        # Biases\n",
    "        self.b = [np.zeros((d, 1)) for d in layer_dims]\n",
    "\n",
    "        # Weighted sums\n",
    "        self.z = [np.zeros((d, 1)) for d in layer_dims]\n",
    "\n",
    "        # Weighted sum activation values\n",
    "        self.a = [np.zeros((d, 1)) for d in layer_dims]\n",
    "\n",
    "\n",
    "    def ff(self, x):\n",
    "        for l in range(len(self.W)):\n",
    "            prev_a = (self.a[l-1] if l > 0 else x)\n",
    "\n",
    "            # Get weighted sum of activations from previous layer into each node\n",
    "            # of current layer\n",
    "            self.z[l] = np.matmul(self.W[l], prev_a) + self.b[l]\n",
    "\n",
    "            # Apply activation function to weighted sum\n",
    "            self.a[l] = sigmoid(self.z[l])\n",
    "\n",
    "        return self.a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36098376],\n",
       "       [0.65881161]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NN(2, [3, 2])\n",
    "\n",
    "x = np.array([[1], [4]])\n",
    "net.ff(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, input_dim, layer_dims):\n",
    "        dims = [input_dim] + layer_dims\n",
    "\n",
    "        # Weights\n",
    "        self.W = []\n",
    "        for i in range(1, len(dims)):\n",
    "            # Begin with random weights\n",
    "            self.W.append( normal(size=(dims[i], dims[i-1])) )\n",
    "\n",
    "        # Biases\n",
    "        self.b = [normal(size=(d, 1)) for d in layer_dims]\n",
    "\n",
    "        # Weighted sums\n",
    "        self.z = [np.zeros((d, 1)) for d in layer_dims]\n",
    "\n",
    "        # Weighted sum activation values\n",
    "        self.a = [np.zeros((d, 1)) for d in layer_dims]\n",
    "\n",
    "\n",
    "    def ff(self, x):\n",
    "        for l in range(len(self.W)):\n",
    "            prev_a = (self.a[l-1] if l > 0 else x)\n",
    "\n",
    "            # Get weighted sum of activations from previous layer into each node\n",
    "            # of current layer\n",
    "            self.z[l] = np.matmul(self.W[l], prev_a) + self.b[l]\n",
    "\n",
    "            # Apply activation function to weighted sum\n",
    "            self.a[l] = sigmoid(self.z[l])\n",
    "\n",
    "        return self.a[-1]\n",
    "\n",
    "\n",
    "    def backprop(self, x, y, alpha=0.05):\n",
    "        # Calculate gradients for weights and biases\n",
    "        # with respect to loss function\n",
    "        dC_dz = copy(self.z)\n",
    "        dC_dW = copy(self.W)\n",
    "        dC_db = copy(self.b)\n",
    "        for i in reversed(range(len(self.W))):\n",
    "            if i == len(self.W) - 1:\n",
    "                # Derivative of quadratic loss function (cost) relative\n",
    "                # to the activation values (prediction)\n",
    "                dC_da = (y - self.a[-1])\n",
    "\n",
    "                # BP 1: derivative of cost relative weighted sum\n",
    "                #       (dC/da[-1]) * (da[-1]/dz[-1]) = dC/dz[-1]\n",
    "                dC_dz[i] = np.multiply(dC_da, sigmoid_(self.z[i])) # Element-wise multiplication\n",
    "            else:\n",
    "                # BP 2: derivative of cost function relative to weighted sum\n",
    "                #       dC/da[i]\n",
    "                #        = (dC/dz[i+1]) * (dz[i+1]/da[i]) --> this ratio is the weight!\n",
    "                #        = (dC/dz[i+1]) * W[i+1] --> this gets transposed and placed on left to line everything up\n",
    "                #\n",
    "                #       dC/dz[i]\n",
    "                #        = (dC/da[i]) * (da[i]/dz[i])\n",
    "                #        = (dC/da[i]) * activation'(dz[i])\n",
    "                dC_dz[i] = np.multiply(np.matmul(self.W[i+1].T, dC_dz[i+1]), sigmoid_(self.z[i]))\n",
    "\n",
    "            # BP 3\n",
    "            dC_db[i] = dC_dz[i]\n",
    "\n",
    "            # BP 4\n",
    "            if i == 0:\n",
    "                dC_dW[i] = np.matmul(dC_dz[i], x.T)\n",
    "            else:\n",
    "                dC_dW[i] = np.matmul(dC_dz[i], self.a[i-1].T)\n",
    "\n",
    "        # Update weights and baises\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] += dC_dW[i] * alpha\n",
    "            self.b[i] += dC_db[i] * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NN(2, [3, 2])\n",
    "\n",
    "x = np.array([[1], [4]])\n",
    "net.ff(x)\n",
    "\n",
    "y = np.array([[0], [1]])\n",
    "net.backprop(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = np.zeros((iris.target.shape[0], 3))\n",
    "for i, idx in enumerate(iris.target):\n",
    "    Y[i, idx] = 1\n",
    "\n",
    "def test(net, X, Y):\n",
    "    correct = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i].reshape(X.shape[1], 1)\n",
    "        pred = np.argmax(net.ff(x))\n",
    "        correct += (pred == iris.target[i])\n",
    "\n",
    "    return correct / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.6666666666666666\n",
      "200 0.7333333333333333\n",
      "300 0.7533333333333333\n",
      "400 0.7866666666666666\n",
      "500 0.7933333333333333\n",
      "600 0.8\n",
      "700 0.8\n",
      "800 0.8\n",
      "900 0.8\n",
      "1000 0.8066666666666666\n"
     ]
    }
   ],
   "source": [
    "net = NN(X.shape[1], [2, 3])\n",
    "\n",
    "iterations = 1000\n",
    "for i in range(1, iterations+1):\n",
    "    for j in range(X.shape[0]):\n",
    "        x = X[j].reshape(X.shape[1], 1)\n",
    "        net.ff(x)\n",
    "        net.backprop(x, Y[j].reshape(3, 1), alpha=0.01)\n",
    "\n",
    "    if i % (iterations // 10) == 0:\n",
    "        print(i, test(net, X, Y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
